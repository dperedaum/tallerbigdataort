{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50936508-953b-40cf-aa07-d29d7137c329",
   "metadata": {},
   "source": [
    "# Generación de BigTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21132465-4724-4e99-b4d1-a34aa649cdbe",
   "metadata": {},
   "source": [
    "Se opto como mecanismo de modelado de datos para la capa de visualización la estrategía de BigTable. Se genero una consulta en Spark SQL uniendo todas las tablas, se guardo en parquet en zona curated del datalake y por último se genero una nueva tabla externa en Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c16524-ad60-4539-bc2a-a88dc7077389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-22T23:37:08,031 WARN [Thread-4] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"crear_one_big_table\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a66302-a527-45fa-96d1-044aed24aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigtable = spark.sql(\"\"\"\n",
    "    select \n",
    "        ae.id_sensor as act_eve_id_sensor, \n",
    "        ae.pulse_rate as act_eve_pulse_rate,\n",
    "        ae.id_actividad as act_eve_id_actividad,\n",
    "        ae.longitud as act_eve_longitud,\n",
    "        ae.latitud as act_eve_latitud,\n",
    "        ae.fecha_hora as act_eve_fecha_hora, \n",
    "        a.id_usuario as act_id_usuario, \n",
    "        a.id_dispositivo as act_id_dispositivo,\n",
    "        a.id_plan as act_id_plan,\n",
    "        a.tipo_actividad as act_tipo_actividad,\n",
    "        a.pais as act_pais,\n",
    "        a.longitud as act_longitud,\n",
    "        a.latitud as act_latitud, \n",
    "        a.duracion as act_duracion,\n",
    "        a.fecha_hora as act_fecha_hora,\n",
    "        u.nombre as usu_nombre, \n",
    "        u.apellido as usu_apellido,\n",
    "        u.email as usu_email,\n",
    "        u.sexo as usu_sexo,\n",
    "        u.pais as usu_pais, \n",
    "        u.actividad as usu_actividad,\n",
    "        u.subscripcion as usu_subscripcion,\n",
    "        u.fecha_subs as usu_fecha_subs,\n",
    "        u.fecha_renov as usu_fecha_renov,\n",
    "        u.ultimo_pago as usu_ultimo_pago,\n",
    "        d.model as dis_model,\n",
    "        d.brand as dis_brand,\n",
    "        d.operating_system as dis_operating_system,\n",
    "        pe.tipo as pla_tipo,\n",
    "        pe.plan_duracion as pla_plan_duracion,\n",
    "        pe.instrucciones as pla_instrucciones, \n",
    "        pe.objetivo as pla_objetivo        \n",
    "    from \n",
    "        datalake.actividades_eventos ae \n",
    "            INNER JOIN datalake.actividades a ON (ae.id_actividad = a.id_actividad)\n",
    "            INNER JOIN datalake.usuarios u ON (a.id_usuario = u.id_usuario) \n",
    "            INNER JOIN datalake.dispositivos d ON (a.id_dispositivo = d.id_dispositivo)\n",
    "            INNER JOIN datalake.planes_de_entrenamiento pe ON (a.id_plan = pe.id_plan and pe.id_usuario = a.id_usuario)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a2da78-e604-48ed-8f26-e8ec5459cf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- act_eve_id_sensor: string (nullable = true)\n",
      " |-- act_eve_pulse_rate: integer (nullable = true)\n",
      " |-- act_eve_id_actividad: string (nullable = true)\n",
      " |-- act_eve_longitud: float (nullable = true)\n",
      " |-- act_eve_latitud: float (nullable = true)\n",
      " |-- act_eve_fecha_hora: timestamp (nullable = true)\n",
      " |-- act_id_usuario: string (nullable = true)\n",
      " |-- act_id_dispositivo: string (nullable = true)\n",
      " |-- act_id_plan: string (nullable = true)\n",
      " |-- act_tipo_actividad: string (nullable = true)\n",
      " |-- act_pais: string (nullable = true)\n",
      " |-- act_longitud: float (nullable = true)\n",
      " |-- act_latitud: float (nullable = true)\n",
      " |-- act_duracion: integer (nullable = true)\n",
      " |-- act_fecha_hora: timestamp (nullable = true)\n",
      " |-- usu_nombre: string (nullable = true)\n",
      " |-- usu_apellido: string (nullable = true)\n",
      " |-- usu_email: string (nullable = true)\n",
      " |-- usu_sexo: string (nullable = true)\n",
      " |-- usu_pais: string (nullable = true)\n",
      " |-- usu_actividad: string (nullable = true)\n",
      " |-- usu_subscripcion: string (nullable = true)\n",
      " |-- usu_fecha_subs: date (nullable = true)\n",
      " |-- usu_fecha_renov: date (nullable = true)\n",
      " |-- usu_ultimo_pago: date (nullable = true)\n",
      " |-- dis_model: string (nullable = true)\n",
      " |-- dis_brand: string (nullable = true)\n",
      " |-- dis_operating_system: string (nullable = true)\n",
      " |-- pla_tipo: string (nullable = true)\n",
      " |-- pla_plan_duracion: float (nullable = true)\n",
      " |-- pla_instrucciones: string (nullable = true)\n",
      " |-- pla_objetivo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bigtable.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7196bff8-2811-4ba6-9219-9135197ce5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-19T16:32:49,112 WARN [Thread-4] org.apache.spark.sql.catalyst.util.package - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_bigtable.write.mode(\"overwrite\").parquet(\"/datalake/curated/bigtable_data_fitness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c141a1d2-450c-4c70-80ff-adf7816c4abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-19T15:10:40,490 INFO [Thread-4] org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController - Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a2450c7e-11e4-47d5-922d-d4ba01540fbd, clientType=HIVECLI]\n",
      "2023-11-19T15:10:40,502 WARN [Thread-4] org.apache.hadoop.hive.ql.session.SessionState - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "2023-11-19T15:10:40,536 INFO [Thread-4] hive.metastore - Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "2023-11-19T15:10:41,250 INFO [Thread-4] hive.metastore - Closed a connection to metastore, current connections: 0\n",
      "2023-11-19T15:10:41,252 INFO [Thread-4] hive.metastore - Trying to connect to metastore with URI thrift://localhost:9083\n",
      "2023-11-19T15:10:41,430 INFO [Thread-4] hive.metastore - Opened a connection to metastore, current connections: 1\n",
      "2023-11-19T15:10:41,434 INFO [Thread-4] hive.metastore - Connected to metastore.\n",
      "2023-11-19T15:10:44,489 INFO [Thread-4] hive.metastore - Trying to connect to metastore with URI thrift://localhost:9083\n",
      "2023-11-19T15:10:44,490 INFO [Thread-4] hive.metastore - Opened a connection to metastore, current connections: 2\n",
      "2023-11-19T15:10:44,494 INFO [Thread-4] hive.metastore - Connected to metastore.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS datalake.bigtable_data_fitness (\n",
    "    act_eve_id_sensor STRING,\n",
    "    act_eve_pulse_rate INT,\n",
    "    act_eve_id_actividad STRING,\n",
    "    act_eve_longitud FLOAT,\n",
    "    act_eve_latitud FLOAT,\n",
    "    act_eve_fecha_hora TIMESTAMP,\n",
    "    act_id_usuario STRING,\n",
    "    act_id_dispositivo STRING,\n",
    "    act_id_plan STRING,\n",
    "    act_tipo_actividad STRING,\n",
    "    act_pais STRING,\n",
    "    act_longitud FLOAT,\n",
    "    act_latitud FLOAT,\n",
    "    act_duracion INT,\n",
    "    act_fecha_hora TIMESTAMP,\n",
    "    usu_nombre STRING,\n",
    "    usu_apellido STRING,\n",
    "    usu_email STRING,\n",
    "    usu_sexo STRING,\n",
    "    usu_pais STRING,\n",
    "    usu_actividad STRING,\n",
    "    usu_subscripcion STRING,\n",
    "    usu_fecha_subs DATE,\n",
    "    usu_fecha_renov DATE,\n",
    "    usu_ultimo_pago DATE,\n",
    "    dis_model STRING,\n",
    "    dis_brand STRING,\n",
    "    dis_operating_system STRING,\n",
    "    pla_tipo STRING,\n",
    "    pla_plan_duracion FLOAT,\n",
    "    pla_instrucciones STRING,\n",
    "    pla_objetivo STRING\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '/datalake/curated/bigtable_data_fitness';\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e769d29-6a43-415f-8afd-f78eff117595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM  datalake.bigtable_data_fitness\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
